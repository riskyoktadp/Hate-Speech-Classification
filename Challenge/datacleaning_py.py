# -*- coding: utf-8 -*-
"""DataCleaning.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Db5ZnW3dC46Yio-lXPe74QMZ22CRufzw
"""

# import libraries

import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import RegexpTokenizer
nltk.download('punkt')
import re
import string

"""**Load** **Data**"""

tweet_df = pd.read_csv('/content/indonesian_hate_speech.csv',encoding='latin-1')
alay_dict = pd.read_csv('/content/new_kamusalay.csv', names = ['original','replacement'], encoding='latin-1')
alay_dict = alay_dict.rename(columns={0: 'original', 
                                      1: 'replacement'})

tweet_df.head()

tweet_df.shape

print("Shape: ", alay_dict.shape)
alay_dict.head(5)

tweet_df.info()

"""**Tokenize**"""

# stopwords
nltk.download('stopwords')

# cleansing 

def tokenizeWords(s, remove_punctuation=True):
    if remove_punctuation == True:
        tokenizer = RegexpTokenizer('\w+|\$[\d\.]+|\S+')
        clean_words = tokenizer.tokenize(s)
    else:
        clean_words = nltk.word_tokenize(s)
    return clean_words

# Tokenize words
tweet_df['tokens'] = tweet_df['Tweet'].apply(tokenizeWords)

tweet_df.head()

def removeUselessText(tokens):
    new_tokens = []
    for t in tokens:
              # Remove hashtag
        if not t.startswith(r'#'):

              # Remove spasi
              t = re.sub(r'[^\w]', ' ', t)

              # remove @
              t = re.sub(r'@[^\s]+', '', t)

              # remove urls
              t = re.sub(r'\\/', '/', t) # replace escaped character
              t = re.sub(r'(https?://\S+)', '', t) # remove urls

              # remove special character and number
              t = re.sub(r'[^a-zA-Z\s]', '', t)

              # remove RT and rt
              t =  re.sub(r'rt',' ',t) # Remove every retweet symbol
              t =  re.sub(r'RT',' ',t) # Remove every retweet symbol

              # remove user and USER
              t = re.sub(r'user',' ',t) # Remove every username
              t = re.sub(r'USER',' ',t) # Remove every username

              # remove emoticon
              t = re.sub(r'x..', ' ', t)
              t = re.sub(r' n ', ' ', t)
              t = re.sub(r'\\+', ' ', t)
              t = re.sub(r'  +', ' ', t)
              t = re.sub(r'x[a-z0-9]{2}',' ', t)
              t = re.sub(r'\ |\?|\.|\!|\/|\;|\:', '', t)

              new_tokens.append(t)

    return [token for token in new_tokens if token]

tweet_df['no_useless'] = tweet_df['tokens'].apply(removeUselessText)

tweet_df.head()

tweet_df.tail()

# remove stopwords

def removeStopWords(tokens, min_len=3):
    from spacy.lang.id.stop_words import STOP_WORDS

    return [t for t in tokens if t not in STOP_WORDS and len(t)>min_len]

tweet_df['tweet_ready'] = tweet_df['no_useless'].apply(removeStopWords)

tweet_df.head()

# Combine cleaned text into one string


tweet_df['tweet_cleaned'] = tweet_df['tweet_ready'].apply(lambda x: ' '.join(x))
tweet_df['hs_class'] = tweet_df['HS'].apply(lambda x: 1 if x == 'HS' else 0)

tweet_df.head()

tweet_df.to_csv("tweet_cleaned")
print()

tweet_df.head()

from google.colab import data_table
data_table.enable_dataframe_formatter()

from google.colab import data_table
from vega_datasets import data

data_table.enable_dataframe_formatter()